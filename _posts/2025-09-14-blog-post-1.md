---
title: 'Anatomy of a Lock-Free Algorithm'
date: 2025-09-14
permalink: /posts/anatomy-of-a-lock-free-algorithm/
tags:
  - lock free data structures
  - graph algorithms
---

In this post, I'll describe lock-free implementations of 2 graph algorithms: (i) shortest path (ii) variable length
recursive join queries. This is the first part of two blog posts and is meant to be companion to my [VLDB '25 paper](https://arxiv.org/abs/2508.19379)
on parallelizing recursive queries in graph databases. The paper focuses more on efficient scheduling policies at the database physical operator level.
But we did not really get a chance to describe _how we parallelize the graph algorithms itself_ (section 4.2 in the paper provides some details).

// add a table right in the beginning to convey the importance / significance of work

Since *most* people don't have a background in graph database algorithms or what "lock-free" even means, I have a background section
on this. The two algorithms I describe here are on the opposite end of the spectrum. The (unweighted) shortest path query returning only
the path lengths is relatively easy in terms of what we need to keep track of and how to parallelize. The variable length path query (walk semantics) returning paths
is the most expensive (compute & memory wise) and complicated query you could ask a graph database to execute. At the end I also have a section
on what challenges we faced while running these queries, and how beneficial lock-free algorithms really are in practice. 


# Table of Contents
1. [Background](#Background)
2. [Shortest Path](#Shortest Path)
3. [Variable Length Path](#Variable Length Path)
4. [So,what's the catch?](#So,what's the catch?)


## Background

It's 2025, and most graph databases now support Cypher as a query language (finally some consolidation here).
Expressing many-to-many joins (queries with long join query patterns) or recursive queries in Cypher is way more intuitive.
When I say "recursive queries" here, I am referring to queries that involve repeated number of joins until a condition is satisfied.
The condition can be for example, encountering a particular node / row while (shortest path) or reaching a particular join depth (variable length).
In Cypher, if you wanted to write the unweighted shortest path query, and return the path length, this would be the syntax:

```sql
MATCH p = (a:Person)-[r:Knows* SHORTEST 1..30]->(b:Person)
WHERE a.name = 'Alice' AND b.name = 'Bob'  
RETURN length(p)
```

The above query would take the node 'Alice', explore who Alice knows (1 join) then explore their neighbours (2 joins) and keep
going until we hit 'Bob'. We "recursively" keep exploring until we either hit our destination or we hit the upper bound 
(specified after the **SHORTEST** keyword).

```sql
MATCH p = (a:Person)-[r:Knows* 1..10]->(b:Person)    
WHERE a.name = 'Alice' AND b.name = 'Bob'  
RETURN p
```

This query is a bit different, as it does not have the SHORTEST keyword, it will compute all paths even if 'Bob' is encountered 
once. It's the (dreaded) variable length recursive join. There are different semantics for this query (ACYCLIC, TRAIL, SIMPLE) which
I'll explain next. The key difference to note here is the stopping condition of variable length query is hitting the upper bound 
specified, and depending on the semantics the query engine may have to keep track of cycles, which is not the case for the
shortest path query.

Let's take the following graph as an example (we'll keep using this as a running example):

<figure>
  <img src="/images/graph.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;">**Figure a**: Example Graph</figcaption>
</figure>

If a user ran the **SHORTEST** path query from node (1) to node (4), the result would return the path:
(1)-[e1]->(2)-[e3]->(4). If we need all the shortest paths, we could specify **ALL SHORTEST** in our query
and get back (1)-[e2]->(3)-[e4]->(4) as well. 

Now if we ran the variable length kleene star query between (1) and (4), notice that we have a cycle in our graph.
Based on which semantic is followed, the path to be returned would vary:

- ACYCLIC: no node and edge can be repeated 
- TRAIL: no edge can be repeated
- SIMPLE: no limitation on repeating nodes or edges

So, with **SIMPLE** var-len join, we would return (1)-[e1]->(2)-[e3]->(4), (1)-[e1]->(2)-[e5]->(1)-[e1]->(2)-[e3]->(4)
and keep going through this cycle until the upper bound for exploration has been reached.  
For **TRAIL** however, the rules change because (1)-[e1]->(2)-[e5]->(1)-[e1]->(2)-[e3]->(4) repeating edges ([e1] edge).
A valid path would be (1)-[e1]->(2)-[e5]->(1)-[e2]->(3)-[e4]->(4) where we did not repeat any edge, but the node (1) did get repeated.
The **ACYCLIC** semantic is the simplest that restricts repeating edges and nodes, it would eliminate both the above paths 
and just return (1)-[e1]->(2)-[e3]->(4) and (1)-[e2]->(3)-[e4]->(4).  

Now that we have the background about all the different recursive joins, in the following
sections I'll explain our implementation for the lock-free: (i) the shortest path query (returning path length) - 
the easiest algorithm to implement and parallelize and, (ii) the variable length path query with SIMPLE path semantics
(returning paths) - the hardest query to parallelize and get good performance on. 

## Shortest Path

The query we're trying to parallelize here:

```sql
MATCH p = (a:Person)-[r:Knows* SHORTEST 1..30]->(b:Person)
WHERE a.name = 'Alice' AND b.name = 'Bob'  
RETURN length(p)
```

Since this is the unweighted shortest path, the algorithm we have to run is the classic breadth first search.
The queue is the data structure of choice to implement the single threaded BFS algorithm.
The parallel version however follows the [_bulk synchronous paradigm_](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel) where we have a barrier between moving from 1 BFS 
level / frontier to the next frontier. The unit of work that Threads receive is a portion of the current BFS level / frontier being
explored, each Thread explores its partition's neighbours and we call this _frontier parallelism_ in the paper. 

<figure>
  <img src="/images/parallel_bfs.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;">**Figure b**: Parallel BFS Overview</figcaption>
</figure>

Let's consider all the structures we need to keep track of for our BFS algorithm:

(i) a _global_ data structure, to keep track of which nodes were visited already (`global_visited`)     
(ii) a _global_ data structure, to keep track of nodes in the current BFS level (`curr_frontier`)     
(iii) a _global_ data structure, to keep track of path lengths of destination nodes (`path_length`)   

The simplest representation for (i)-(iii) is having an array of size _(total nodes)_ in the graph, and performing atomic
operations to ensure all updates are visible to every thread, and this is what we used. In addition, we also need to keep
track of nodes that were just encountered / visited, and will be part of the next BFS level. Again, we use an array for this
(`next_frontier`) and simply switch the reference (pointers) between the `cur_frontier` and `next_frontier` during the barrier
phase.    
During traversal of each level by the Threads, we need to ensure reads / writes to the shared structures occur safely.
We could slap a lock on each of the arrays, or partition them by buckets and attach a lock to each bucket, all of which
would slow down the parallel BFS dramatically. This is where the atomic `compare_and_exchange` (also called `compare_and_swap`, CAS in short),
atomic `load` and atomic `store` operations come in handy.   

<figure>
  <img src="/images/sp_cas_parallel.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: center;">**Figure c**: Concurrent Access by Threads</figcaption>
</figure>

Consider the above scenario, where **Thread 1** and **Thread 2** encounter the same node
while exploring the neighbours of their partition. Both of them would try to confirm if the node
is unvisited by reading the state from the `global_visited` array, and then try to update it to `VISITED`.
To ensure this operation happens safely, `atomic_load` is used to read the value safely. Then, to ensure 
only 1 Thread succeeds in writing to the `global_visited` array, an `atomic_cas` operation is used. The Thread
that "wins" (succeeds in writing to the neighbour's position), proceeds to write the length and update the next 
frontier (indicating this node is part of the next frontier). The pseudocode of this is as follows:

```c++
visited_nbrs(nbr_node) {
    state = atomic_read(global_visited[nbr_node])
    if (state == NOT_VISITED) {
        if (atomic_cas(&global_visited[nbr_node], state, VISITED) {
           atomic_store(&next_frontier[nbr_node], 1)
           atomic_store(path_length[nbr_node], current_level + 1)
        }
    }
}
```

Each Thread executes this sub-routine for every neighbour it explores until it exhausts its frontier partition. The overall
idea is very simple to implement. What are the drawbacks of this approach ? Stick till the end, I'll cover them in the final section. 

## Variable Length Path

The query we're trying to parallelize here is:

```sql
MATCH p = (a:Person)-[r:Knows* 1..10]->(b:Person)
WHERE a.name = 'Alice' AND b.name = 'Bob'  
RETURN p
```

We'll have to perform the parallel BFS algorithm in this case as well, but the key difference between this and the previous query is 
having to keep track of cycles. If we take a look at our example graph again:

<figure>
  <img src="/images/graph.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;"></figcaption>
</figure>

Let's enumerate what each BFS level would look like, starting from the source node (1):

```
Level 0: (1)
Level 1: (2), (3)
Level 2: (1), (4)
Level 3: (2), (3)
...
```

So now we have to keep track of multiple occurrences of a node across different levels. Having a single BFS level / length
associated with a node is not enough. How about the paths that we have to enumerate ? Because this query returns paths:

```
Length 1: (1)-[e1]->(2), (1)-[e2]->(3)
Length 2: (1)-[e1]->(2)-[e3]->(4), (1)-[e2]->(3)-[e4]->(4), (1)-[e1]->(2)-[e5]->(1)
Length 3: (1)-[e1]->(2)-[e5]->(1)-[e1]->(2), (1)-[e1]->(2)-[e5]->(1)-[e2]->(3)
...
```

When we enumerate the paths, we notice that there is a lot of **_redundancy_** once we flatten out the paths. 
The redundancy comes from nodes that are connected to multiple outgoing nodes and have to be repeated for each path when
producing the resulting paths. So the goal while computing a query such as this is to avoid _materializing large 
intermediate results_, keep those nodes that have been repeated across multiple paths only once and have the paths point back to them.
Let's visualize the paths for different lengths and understand how we can represent them in a compact manner. 
Consider an array of all nodes in the graph and map out the paths of `Length 1`:

<figure>
  <img src="/images/var_len_len_1.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;"></figcaption>
</figure>

Each position in the array points to a block that contains information such as which BFS level it was encountered at,
and the (edge ID, source node ID) that led to this node. Now, when we visualize the paths one level deeper:

<figure>
  <img src="/images/var_len_len_2.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;"></figcaption>
</figure>

This representation avoids repeating `Node (2)` while keeping track of the 2 paths of `Length 2` that start from it.
This is useful in practice, especially when having to keep track of paths with cycles because it gives us a way to extract
the paths when needed, but also avoids repeating nodes that are part of multiple paths at the same level. We can easily
determine the paths by backtracking from the destination node to the source node using the (edge ID, source node ID) information
stored in each block. We define 2 structs to store the information encapsulated in the blocks:

```c++
struct EdgeList {
    offset_t edge_offset
    EdgeListAndLevel* src
    EdgeList* next
}

struct EdgeListAndLevel {
    uint8_t bfs_level
    EdgeListAndLevel *next_level
    EdgeList* top
}
```

The `EdgeListAndLevel` struct keeps track of specific levels where a node was encountered, a pointer to the next
(lower) level and a pointer to the list of edges through which the node was encountered at this level. The `EdgeList` struct
keeps track of the edge offset (ID) and a pointer to the source node's `EdgeListAndLevel` struct. Essentially, this is
a linked list that grows two ways - vertically (BFS levels) and horizontally (all edges at a level). The flow chart for
the algorithm works is as follows:

<figure>
  <img src="/images/var_len_cas_parallel.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;"></figcaption>
</figure>

This is sufficiently complex, so let's break it down step by step:

1. Each Thread explores its partition of the current BFS level / frontier as before
2. Unlike shortest path, in var-len queries nodes need not be marked visited only once, and can be visited multiple
   times at different levels. We still use the `global_visited` array to mark if a node is visited at least once by 
   by any Thread and reset at the end of a BFS level (`NOT_VISITED` changed to `VISITED`). This is step `(1)` in the
   flow chart.
3. Now we need to check if the node was already encountered at the current BFS level. This is step `(2)` in the flow chart.
   If it was not, we create a new `EdgeListAndLevel` struct for this level and try to insert it into the linked list
   of levels for this node using an `atomic_cas` operation. If we succeed, we proceed to step `(3)`. If we fail,
   it means another Thread already inserted a struct for this level concurrently. The Threads delete the struct they created
   and read the correct `EdgeListAndLevel` from the node position in the array.
4. In step `(3)`, we create a new `EdgeList` struct for the edge that led to this node and try to insert it into the linked list of edges
   for this level using an `atomic_cas` operation. If we succeed, we are done processing this neighbour. If we fail, it means another Thread inserted an edge concurrently,
   so we read the updated `top` head pointer of the `EdgeListAndLevel` block and try again until we succeed. This is step `(4)` in the flow chart.
5. Once all neighbours of all nodes in the current frontier are processed, we hit the barrier, swap the frontiers and move to the next BFS level.

Step `(3)` can be slightly optimized by reducing the number of memory allocations. Instead of allocating a new `EdgeList` struct for every neighbour,
we can make a single large allocation for the entire partition of neighbours that a Thread is processing and make the 
`EdgeList` structs point to the appropriate offset in this large allocation. We call these large `EdgeList` allocations
`EdgeListSegments`. This optimization however does not apply to the `EdgeListAndLevel` structs, since they are created
only once per node per level. For storing the `EdgeListAndLevel` structs per node, we use an array of pointers of size 
_(total nodes)_ in the graph, and perform atomic operations to ensure all updates are visible to every thread. 
The pseudocode for processing each neighbour is as follows:

```c++
visited_nbrs(nbr_node, nbr_offset, edge_offset, src_node_level) {
    
    // Step (1) in flow chart
    state = atomic_read(global_visited[nbr_node])
    if (state == NOT_VISITED) {
        if (atomic_cas(&global_visited[nbr_node], state, VISITED)) {
            atomic_store(&next_frontier[nbr_node], 1)
        }
    }

    // Step (2) in flow chart
    node_level = NodeEdgeListAndLevels[nbr_node]
    if (node_level == NULL || node_level.bfs_level != current_level) {
        new_level = new EdgeListAndLevel(current_level, node_level, NULL)
        if (!atomic_cas(&NodeEdgeListAndLevels[nbr_node], node_level, new_level)) {
            delete new_level
            node_level = atomic_load(&NodeEdgeListAndLevels[nbr_node])
        } else {
            node_level = new_level
        }
    }
    
    // Step (3) in flow chart
    EdgeListSegment.edge_list_block[nbr_offset].edge_offset = edge_offset
    EdgeListSegment.edge_list_block[nbr_offset].src = src_node_level
    curr_top = atomic_load(&node_level.top)
    EdgeListSegment.edge_list_block[nbr_offset].next = curr_top
    
    // Step (4) in flow chart
    while (!atomic_cas(&node_level.top, curr_top, &EdgeListSegment.edge_list_block[nbr_offset])) {
        curr_top = atomic_load(&node_level.top)
        EdgeListSegment.edge_list_block[nbr_offset].next = curr_top
    }
}
```

This covers technical details of the lock-free variable length query algorithm for returning paths. Now lets look at some
of the caveats of these techniques and what challenges we faced while implementing these algorithms in practice.

## So,what's the catch?

I just described two "fancy" algorithms that are fast, lock-free and scale well with more threads. But what's the catch ?
Are there any drawbacks to these techniques ? There's quite a few actually, and I'll go through them one by one:

- **Memory Consumption**: If it was not clear already, both algorithms use a lot of large arrays of size _(total nodes)_
  in the graph. This is not a problem for small graphs, but for large graphs with billions of nodes, this can be a problem. 
  Some of the arrays can be optimized to be bitmaps such as the `global_visited` and `next_Frontier` array, but others 
  such as the `NodeEdgeListAndLevels` array store a pointer per node which cannot be reduced. The problem with using 
  bitmaps is that atomic operations on bits are not natively supported by hardware, so we have to use `atomic_cas` on 
  the entire byte / word, leading to more contention. The variable length query uses a lot more memory because of the 
  linked list structures it maintains per node that grows vertically and horizontally.

- **Memory Allocations**: The variable length query algorithm needs to allocate a lot of small structures 
  (`EdgeListAndLevel`, `EdgeList`) during traversal. Even with the optimization of allocating large `EdgeListSegments`,
  to reduce the `EdgeList` allocations, the number of allocations remains high across multiple threads and the 
  `EdgeListAndLevel` still requires a new allocation per node per level (less compared to `EdgeList` allocations, but 
  still significant).    
  We initially benchmarked our variable length algorithm in Kùzu using standard `malloc` for the `LiveJournal` graph for
  a query returning ~ 140k paths and compared it to using `jemalloc` (a high performance memory allocator):


<div style="display: flex; gap: 2rem; justify-content: center;">

  <div style="text-align: center;">
  <strong>Table 1. Runtime</strong>

|          |    malloc     |    jemalloc     |
|----------|:-------------:|:---------------:|
| Cold Run |  58 seconds   |   27 seconds    |
| Warm Run |  30 seconds   |   26 seconds    |
  </div>

  <div style="text-align: center;">
  <strong>Table 2. Memory Usage</strong>

|             |    malloc     |    jemalloc     |
|-------------|:-------------:|:---------------:|
| Peak        |     75 GB     |      47 GB      |
| Post-Query  |     70 GB     |      11 GB      |
  </div>

</div>

With `jemalloc` the cold start runtime is **2x faster** and peak memory usage is **1.6x lower**. The post-query memory footprint
is way lower with `jemalloc` because it returns memory to the OS more aggressively than standard `malloc`, which does not
release memory back to the OS. The key takeaway here is that **_using a high performance memory allocator is crucial
for this algorithm and lock-free algorithms in general to be performant in practice_**. You can write the most optimal
lock-free algorithm, but if your memory allocator is not up to the task, you will not see the benefits in practice.

There's a simple way to reduce the memory consumption of both algorithms, by reducing the concurrency level.
When we limit the number of concurrent sources running in parallel to only 1 src node, the same query runs in **62 seconds**
and consumes _only 5 GB peak memory_ (using `jemalloc`). The trade-off here is that the runtime is 2.3x slower.



