---
title: 'Anatomy of a Lock-Free Algorithm'
date: 2025-09-14
permalink: /posts/anatomy-of-a-lock-free-algorithm/
tags:
  - lock free data structures
  - graph algorithms
---

In this post, I'll describe lock-free implementations of 2 graph algorithms: (i) shortest path (ii) variable length
recursive join queries. This is the first part of two blog posts and is meant to be companion to my [VLDB '25 paper](https://arxiv.org/abs/2508.19379)
on parallelizing recursive queries in graph databases. The paper focuses more on efficient scheduling policies at the database physical operator level.
But we did not really get a chance to describe _how we parallelize the graph algorithms itself_ (section 4.2 in the paper provides some details).

// add a table right in the beginning to convey the importance / significance of work

Since *most* people don't have a background in graph database algorithms or what "lock-free" even means, I have a background section
on this. The two algorithms I describe here are on the opposite end of the spectrum. The (unweighted) shortest path query returning only
the path lengths is relatively easy in terms of what we need to keep track of and how to parallelize. The variable length path query (walk semantics) returning paths
is the most expensive (compute & memory wise) and complicated query you could ask a graph database to execute. At the end I also have a section
on what challenges we faced while running these queries, and how beneficial lock-free algorithms really are in practice. 


# Table of Contents
1. [Background](#Background)
2. [Shortest Path](#Shortest Path)
3. [Variable Length Path](#Variable Length Path)
4. [So,what's the catch?](#So,what's the catch?)


## Background

It's 2025, and most graph databases now support Cypher as a query language (finally some consolidation here).
Expressing many-to-many joins (queries with long join query patterns) or recursive queries in Cypher is way more intuitive.
When I say "recursive queries" here, I am referring to queries that involve repeated number of joins until a condition is satisfied.
The condition can be for example, encountering a particular node / row while (shortest path) or reaching a particular join depth (variable length).
In Cypher, if you wanted to write the unweighted shortest path query, and return the path length, this would be the syntax:

```sql
MATCH p = (a:Person)-[r:Knows* SHORTEST 1..30]->(b:Person)
WHERE a.name = 'Alice' AND b.name = 'Bob'  
RETURN length(p)
```

The above query would take the node 'Alice', explore who Alice knows (1 join) then explore their neighbours (2 joins) and keep
going until we hit 'Bob'. We "recursively" keep exploring until we either hit our destination or we hit the upper bound 
(specified after the **SHORTEST** keyword).

```sql
MATCH p = (a:Person)-[r:Knows* 1..10]->(b:Person)    
WHERE a.name = 'Alice' AND b.name = 'Bob'  
RETURN p
```

This query is a bit different, as it does not have the SHORTEST keyword, it will compute all paths even if 'Bob' is encountered 
once. It's the (dreaded) variable length recursive join. There are different semantics for this query (ACYCLIC, TRAIL, SIMPLE) which
I'll explain next. The key difference to note here is the stopping condition of variable length query is hitting the upper bound 
specified, and depending on the semantics the query engine may have to keep track of cycles, which is not the case for the
shortest path query.

Let's take the following graph as an example (we'll keep using this as a running example):

<figure>
  <img src="/images/graph.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;">**Figure a**: Example Graph</figcaption>
</figure>

If a user ran the **SHORTEST** path query from node (1) to node (4), the result would return the path:
(1)-[e1]->(2)-[e3]->(4). If we need all the shortest paths, we could specify **ALL SHORTEST** in our query
and get back (1)-[e2]->(3)-[e4]->(4) as well. 

Now if we ran the variable length kleene star query between (1) and (4), notice that we have a cycle in our graph.
Based on which semantic is followed, the path to be returned would vary:

- ACYCLIC: no node and edge can be repeated 
- TRAIL: no edge can be repeated
- SIMPLE: no limitation on repeating nodes or edges

So, with **SIMPLE** var-len join, we would return (1)-[e1]->(2)-[e3]->(4), (1)-[e1]->(2)-[e5]->(1)-[e1]->(2)-[e3]->(4)
and keep going through this cycle until the upper bound for exploration has been reached.  
For **TRAIL** however, the rules change because (1)-[e1]->(2)-[e5]->(1)-[e1]->(2)-[e3]->(4) repeating edges ([e1] edge).
A valid path would be (1)-[e1]->(2)-[e5]->(1)-[e2]->(3)-[e4]->(4) where we did not repeat any edge, but the node (1) did get repeated.
The **ACYCLIC** semantic is the simplest that restricts repeating edges and nodes, it would eliminate both the above paths 
and just return (1)-[e1]->(2)-[e3]->(4) and (1)-[e2]->(3)-[e4]->(4).  

Now that we have the background about all the different recursive joins, in the following
sections I'll explain our implementation for the lock-free: (i) the shortest path query (returning path length) - 
the easiest algorithm to implement and parallelize and, (ii) the variable length path query with SIMPLE path semantics
(returning paths) - the hardest query to parallelize and get good performance on. 

## Shortest Path

The query we're trying to parallelize here:

```sql
MATCH p = (a:Person)-[r:Knows* SHORTEST 1..30]->(b:Person)
WHERE a.name = 'Alice' AND b.name = 'Bob'  
RETURN length(p)
```

Since this is the unweighted shortest path, the algorithm we have to run is the classic breadth first search.
The queue is the data structure of choice to implement the single threaded BFS algorithm.
The parallel version however follows the [_bulk synchronous paradigm_](https://en.wikipedia.org/wiki/Bulk_synchronous_parallel) where we have a barrier between moving from 1 BFS 
level / frontier to the next frontier. The unit of work that Threads receive is a portion of the current BFS level / frontier being
explored, each Thread explores its partition's neighbours and we call this _frontier parallelism_ in the paper. 

<figure>
  <img src="/images/parallel_bfs.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;">**Figure b**: Parallel BFS Overview</figcaption>
</figure>

Let's consider all the structures we need to keep track of for our BFS algorithm:

(i) a _global_ data structure, to keep track of which nodes were visited already (`global_visited`)     
(ii) a _global_ data structure, to keep track of nodes in the current BFS level (`curr_frontier`)     
(iii) a _global_ data structure, to keep track of path lengths of destination nodes (`path_length`)   

The simplest representation for (i)-(iii) is having an array of size _(total nodes)_ in the graph, and performing atomic
operations to ensure all updates are visible to every thread, and this is what we used. In addition, we also need to keep
track of nodes that were just encountered / visited, and will be part of the next BFS level. Again, we use an array for this
(`next_frontier`) and simply switch the reference (pointers) between the `cur_frontier` and `next_frontier` during the barrier
phase.    
During traversal of each level by the Threads, we need to ensure reads / writes to the shared structures occur safely.
We could slap a lock on each of the arrays, or partition them by buckets and attach a lock to each bucket, all of which
would slow down the parallel BFS dramatically. This is where the atomic `compare_and_exchange` (also called `compare_and_swap`, CAS in short),
atomic `load` and atomic `store` operations come in handy.   

<figure>
  <img src="/images/img.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: center;">**Figure c**: Concurrent Access by Threads</figcaption>
</figure>

Consider the above scenario, where **Thread 1** and **Thread 2** encounter the same node
while exploring the neighbours of their partition. Both of them would try to confirm if the node
is unvisited by reading the state from the `global_visited` array, and then try to update it to `VISITED`.
To ensure this operation happens safely, `atomic_load` is used to read the value safely. Then, to ensure 
only 1 Thread succeeds in writing to the `global_visited` array, an `atomic_cas` operation is used. The Thread
that "wins" (succeeds in writing to the neighbour's position), proceeds to write the length and update the next 
frontier (indicating this node is part of the next frontier). The pseudocode of this is as follows:

```c++
visited_nbrs(nbr_node) {
    state = atomic_read(global_visited[nbr_node])
    if (state == NOT_VISITED) {
        if (atomic_cas(&global_visited[nbr_node], state, VISITED) {
           atomic_store(&next_frontier[nbr_node], 1)
           atomic_store(path_length[nbr_node], current_level + 1)
        }
    }
}
```

Each Thread executes this sub-routine for every neighbour it explores until it exhausts its frontier partition. The overall
idea is very simple to implement. What are the drawbacks of this approach ? Stick till the end, I'll cover them in the final section. 

## Variable Length Path

The query we're trying to parallelize here is:

```sql
MATCH p = (a:Person)-[r:Knows* 1..10]->(b:Person)
WHERE a.name = 'Alice' AND b.name = 'Bob'  
RETURN p
```

We'll have to perform the parallel BFS algorithm in this case as well, but the key difference between this and the previous query is 
having to keep track of cycles. If we take a look at our example graph again:

<figure>
  <img src="/images/graph.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;"></figcaption>
</figure>

Let's enumerate what each BFS level would look like, starting from the source node (1):

```
Level 0: (1)
Level 1: (2), (3)
Level 2: (1), (4)
Level 3: (2), (3)
...
```

So now we have to keep track of multiple occurrences of a node across different levels. Having a single BFS level / length
associated with a node is not enough. How about the paths that we have to enumerate ? Because this query returns paths:

```
Length 1: (1)-[e1]->(2), (1)-[e2]->(3)
Length 2: (1)-[e1]->(2)-[e3]->(4), (1)-[e1]->(2)-[e4]->(4), (1)-[e1]->(2)-[e5]->(1)
Length 3: (1)-[e1]->(2)-[e5]->(1)-[e1]->(2), (1)-[e1]->(2)-[e5]->(1)-[e2]->(3)
...
```

When we enumerate the paths, we notice that there is a lot of _redundancy_ when we flatten out the paths. 
The redundancy is more clear, when we visualize it. If we take an array of all nodes in the graph and map
out the paths of `Length 1`:

<figure>
  <img src="/images/var_len_len_1.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;"></figcaption>
</figure>

Each pointer to the block contains the level it was encountered at, the edge ID and the source node ID.
Now, when we visualize the same one level deeper:

<figure>
  <img src="/images/var_len_len_2.png" alt="Description" style="width: 50%;" />
  <figcaption style="text-align: left;"></figcaption>
</figure>

We notice that this representation is more _compact_, compared to flattening out the paths since 
multiple end nodes [(4), Level 2] are able to point back to their parent blocks which are already saved.



## So,what's the catch?



