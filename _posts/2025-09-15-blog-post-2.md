---
title: 'Down the Rabbit Hole with Perf'
date: 2025-09-15
permalink: /posts/down-the-rabbit-hole-with-perf/
tags:
  - lock free data structures
  - graph algorithms
---

In my previous [post](https://anuchak.github.io/posts/anatomy-of-a-lock-free-algorithm/), I described
the implementation of two lock-free algorithms for recursive queries in graph databases that achieved speedup
through atomic operations and parallel BFS traversals. In this post, I will share some interesting performance
findings I encountered while optimizing both algorithms using Linux `Perf`.

Tuning code for performance is never straightforward. It requires an intricate understanding of the what's
happening under the hood, knowing where to look, and knowing the right tool to use. Linux `Perf` is a powerful
tool for this job - it provides rich metrics about CPU cycles, cache misses, TLB misses, branch predictions, and
more. If you're looking for a deeper dive on `Perf`, I recommend this [tutorial](https://github.com/NAThompson/performance_tuning_tutorial).

This post is divided into four sections. The first section briefly describes the algorithms we will tune for performance,
and the scheduling policy that runs them in Kùzu (more details in the [VLDB '25 paper](https://arxiv.org/abs/2508.19379)).
The next two sections describe interesting performance findings: Section 2 describes how a parameter introduced
too much concurrency leading to performance degradation and Section 3 describes how `Perf` revealed a performance 
bottleneck and how prefetching helps to tackle it. The final section, similar to the previous post, discusses certain
hardware requirements for performing such low-level optimizations.

# Table of Contents
1. [Background](#Background)
2. [The curious case of 'k'](#the-curious-case-of-k)
3. [Prefetching at Lightspeed](#prefetching-at-lightspeed)
4. [So, what's the catch?](#so-whats-the-catch)


## Background

In this section, I will briefly describe the hybrid scheduling policy we implement in Kùzu to parallelize recursive
queries and the variable length recursive query returning paths that I will optimize for performance in this post.

### nTkS Scheduling Policy

The `nTkS` scheduling policy is a hybrid policy that combines vanilla [morsel-driven parallelism](https://db.in.tum.de/~leis/papers/morsels.pdf)
implemented in analytical databases with frontier parallelism (parallel BFS) implemented in graph analytics systems such
as [Ligra](https://jshun.csail.mit.edu/ligra.pdf) and [Pregel](https://15799.courses.cs.cmu.edu/fall2013/static/papers/p135-malewicz.pdf).
Where morsel-driven parallelism assigns Threads to execute on a partition of input tuples (table rows or index entries 
or intermediate results), frontier parallelism assigns Threads to execute on a partition of the graph frontier (vertices).
We combine both these approaches to achieve better performance. The `n` in `nTkS` refers to the 
number of Threads running concurrently and `k` refers to the number of sources whose recursive computation is being performed on.
As an example, consider the following query:

```sql
MATCH p = (a:Person)-[r:Knows* SHORTEST 1..30]->(b:Person)
WHERE a.ID > 10 AND a.ID < 21
RETURN length(p)
```

This query runs the shortest path algorithm starting from _10 source_ vertices (i.e., `a.ID` from 11 to 20).
If we set `n=4` and `k=2`, then at any point in time, 4 threads will be running concurrently, and _exactly 2 sources will
be active_. The threads decide which source to execute on depending on the size of the frontier and how many threads are
already executing on that source. More details about the scheduling policy are in the [paper](https://arxiv.org/abs/2508.19379).

### Variable Length Recursive Query

The variable length recursive query is expressed in Cypher as follows:

```sql
MATCH p = (a:Person)-[r:Knows* 1..5]->(b:Person)
WHERE a.ID = srcID AND b.ID = dstID
RETURN p
```

The above query returns all paths from a source vertex `srcID` to a destination vertex `dstID` with a length between 1 and 5.
I've described the lock-free parallel implementation to execute this query in my previous [post](https://anuchak.github.io/posts/anatomy-of-a-lock-free-algorithm/). 
The key thing to remember is that this algorithm is compute-intensive since it keeps track of all paths (including cycles)
and is a good candidate for performance tuning.

## The curious case of 'k'


## Prefetching at Lightspeed


## So, what's the catch?


