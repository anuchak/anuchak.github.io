---
title: 'Down the Rabbit Hole with Perf'
date: 2025-09-15
permalink: /posts/down-the-rabbit-hole-with-perf/
tags:
  - lock free data structures
  - graph algorithms
---

In my previous [post](https://anuchak.github.io/posts/anatomy-of-a-lock-free-algorithm/), I described
the implementation of two lock-free algorithms for recursive queries in graph databases that achieved speedup
through atomic operations and parallel BFS traversals. In this post, I will share some interesting performance
findings I encountered while optimizing both algorithms using Linux `Perf`.

Tuning code for performance is never straightforward. It requires an intricate understanding of the what's
happening under the hood, knowing where to look, and knowing the right tool to use. Linux `Perf` is a powerful
tool for this job - it provides rich metrics about CPU cycles, cache misses, TLB misses, branch predictions, and
more. If you're looking for a deeper dive on `Perf`, I recommend this [tutorial](https://github.com/NAThompson/performance_tuning_tutorial).

This post is divided into four sections. The first section briefly describes the algorithms we will tune for performance,
and the scheduling policy that runs them in Kùzu (more details in the [VLDB '25 paper](https://arxiv.org/abs/2508.19379)).
The next two sections describe interesting performance findings: Section 2 describes how a parameter introduced
too much concurrency leading to performance degradation and Section 3 describes how `Perf` revealed a performance 
bottleneck and how prefetching helps tackle it. The final section, similar to the previous post, discusses certain
hardware requirements for performing such low-level optimizations.

# Table of Contents
1. [Background](#Background)
2. [The curious case of 'k'](#the-curious-case-of-k)
3. [Prefetching at Lightspeed](#prefetching-at-lightspeed)
4. [So, what's the catch?](#so-whats-the-catch)


## Background

In this section, I will briefly describe the hybrid scheduling policy we implement in Kùzu to parallelize recursive
queries and the variable length recursive query returning paths that I will optimize for performance in this post.

### nTkS Scheduling Policy

The `nTkS` scheduling policy is a hybrid policy that combines vanilla [morsel-driven parallelism](https://db.in.tum.de/~leis/papers/morsels.pdf)
implemented in analytical databases with frontier parallelism (parallel BFS) implemented in graph analytics systems such
as [Ligra](https://jshun.csail.mit.edu/ligra.pdf) and [Pregel](https://15799.courses.cs.cmu.edu/fall2013/static/papers/p135-malewicz.pdf).
Where morsel-driven parallelism assigns Threads to execute on a partition of input tuples (table rows or index entries 
or intermediate results), frontier parallelism assigns Threads to execute on a partition of the graph frontier (vertices).
We combine both these approaches to achieve better performance. The `n` in `nTkS` refers to the 
number of Threads running concurrently and `k` refers to the number of sources whose recursive computation is being performed on.
As an example, consider the following query:

```sql
MATCH p = (a:Person)-[r:Knows* SHORTEST 1..30]->(b:Person)
WHERE a.ID > 10 AND a.ID < 21
RETURN length(p)
```

This query runs the shortest path algorithm starting from _10 source_ vertices (i.e., `a.ID` from 11 to 20).
If we set `n=4` and `k=2`, then at any point in time, 4 threads will be running concurrently, and _exactly 2 sources will
be active_. The threads decide which source to execute on depending on the size of the frontier and how many threads are
already executing on that source. More details about the scheduling policy are in the [paper](https://arxiv.org/abs/2508.19379).

### Variable Length Recursive Query

The variable length recursive query is expressed in Cypher as follows:

```sql
MATCH p = (a:Person)-[r:Knows* 1..5]->(b:Person)
WHERE a.ID = srcID AND b.ID = dstID
RETURN p
```

The above query returns all paths from a source vertex `srcID` to a destination vertex `dstID` with a length between 1 and 5.
I've described the lock-free parallel implementation to execute this query in my previous [post](https://anuchak.github.io/posts/anatomy-of-a-lock-free-algorithm/). 
The key thing to remember is that this algorithm is compute-intensive since it keeps track of all paths (including cycles)
and is a good candidate for performance tuning.

## The curious case of 'k'

// basic outline of the section: explain slightly what 'k' means, its influence on the runtime of the nTkS policy 
// then explain that intuitively, increasing k should give better runtime since more sources running in parallel 
// then provide the datasets we tested on, and the results for the many source query with 32 threads and varying k 
// show that spotify is behaving strangely, explain spotify characteristics, it has high average degree and low diameter 
// meaning for a single source there is a lot of parallelism to exploit already, increasing 'k' meaning running more sources 
// is not helping since we get enough LLC cache hits and throughput on a single source, increasing 'k' leads to 
// cache access interference across sources, leading to performance degradation, show perf evidence of this with LLC-hit rate 
// and LLC throughput increase for other graphs with increasing 'k' but decrease for spotify, key takeaway: how perf 
// helped identify the issue, the cache miss metric is a good metric to look at, especially in the context of of graph 
// workloads because they involve a lot of random memory accesses due to edge traversals

One of the core parameters in the nTkS policy is the number of sources, `k`, that we allow to run in parallel. 
Intuitively, increasing `k` should improve runtime: the more sources we process simultaneously, the better we can utilize
available cores and memory bandwidth.

<br>
<div style="text-align: center;">
  <figure style="display: inline-block; margin: 0;">
    <img src="/images/nTkS_policy.png" alt="Description" style="width: 65%;" />
    <figcaption style="text-align: center; font-size: 1.1rem;"><strong>Figure a</strong>: nTkS Runtime Policy </figcaption>
  </figure>
</div>
<br>

An alternative and more simple scheduling policy would be to simply run a single source at a time and parallelize
with all available threads. Then we don't need to worry about assigning threads to multiple source. However, this approach
leads to underutilization in practice since we are not able to parallelize a single source enough and get enough throughput.
We tested varying `k` on the following real-world + synthetic datasets:

<br>
<div style="display: flex; gap: 3rem; justify-content: center;">

  <div style="text-align: center;">
    <strong>Table: Datasets</strong>
    <table style="border-collapse: collapse; margin: 0 auto; font-size: 1.05rem;">
      <thead>
        <tr>
          <th style="border: 1px solid #ccc; padding: 8px 14px; text-align: center;"></th>
          <th style="border: 1px solid #ccc; padding: 8px 14px; text-align: center;">|V|</th>
          <th style="border: 1px solid #ccc; padding: 8px 14px; text-align: center;">|E|</th>
          <th style="border: 1px solid #ccc; padding: 8px 14px; text-align: center;">Avg Degree</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">LDBC-100</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">448,626</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">19,941,198</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">44</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">LiveJournal</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">4,847,571</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">68,993,773</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">14</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">Spotify</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">3,604,454</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">1,927,482,013</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">535</td>
        </tr>
       <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">Graph500-28</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">121,242,388</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">4,236,163,958</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">35</td>
       </tr>
      </tbody>
    </table>
  </div>
</div>
<br>

We ran a recursive join query, starting from 64 sources, keeping the total threads fixed at 32 threads while varying
`k` from 1 to 32 to see its effect on runtime. The following figure shows the improvement factor in runtime compared to 
`k=1` for each dataset:

<br>
<div style="text-align: center;">
  <figure style="display: inline-block; margin: 0;">
    <img src="/images/nTkS_experiment.png" alt="Description" style="width: 65%;" />
    <figcaption style="text-align: center; font-size: 1.1rem;"><strong>Figure b</strong>: Varying 'k' </figcaption>
  </figure>
</div>
<br>

As we notice from the figure, for most graphs (LDBC, LiveJournal, Graph500), increasing `k` leads to better performance 
with speedups of 2-3x, but for Spotify, increasing `k` actually leads to worse performance! Compared to the other datasets,
Spotify has a very high average degree (535) and this means that even a single source has a lot of parallelism to exploit.
This is where Linux `Perf` helped us identify the issue. We used `perf stat` to collect metrics for our query:

<br>
<div style="display: flex; gap: 3rem; justify-content: center;">

  <div style="text-align: center;">
    <strong>Table 6: Runtime (s) vs LLC Throughput (Tp, Million/s). 64-src workload</strong>
    <table style="border-collapse: collapse; margin: 0 auto; font-size: 1.05rem;">
      <thead>
        <tr>
          <th style="border: 1px solid #ccc; padding: 8px 14px;">Dataset</th>
          <th style="border: 1px solid #ccc; padding: 8px 14px;">Metric</th>
          <th style="border: 1px solid #ccc; padding: 8px 14px;">k=1</th>
          <th style="border: 1px solid #ccc; padding: 8px 14px;">k=2</th>
          <th style="border: 1px solid #ccc; padding: 8px 14px;">k=4</th>
          <th style="border: 1px solid #ccc; padding: 8px 14px;">k=8</th>
          <th style="border: 1px solid #ccc; padding: 8px 14px;">k=16</th>
          <th style="border: 1px solid #ccc; padding: 8px 14px;">k=32</th>
        </tr>
      </thead>
      <tbody>
        <!-- LDBC -->
        <tr>
          <td rowspan="3" style="border: 1px solid #ccc; padding: 8px 14px;">LDBC</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">Time (s)</td>
          <td>4.1</td><td>3.3</td><td>2.3</td><td>1.5</td><td>1.3</td><td>1.2</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">LLC Tp</td>
          <td>10.9</td><td>11.4</td><td>13.9</td><td>19.4</td><td>23.6</td><td>23.9</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">CPU %</td>
          <td>66</td><td>78</td><td>85</td><td>88</td><td>95</td><td>98</td>
        </tr>
        <tr>
          <td rowspan="3" style="border: 1px solid #ccc; padding: 8px 14px;">LJ</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">Time (s)</td>
          <td>37.5</td><td>31.2</td><td>22.6</td><td>13.5</td><td>10.3</td><td>9.7</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">LLC Tp</td>
          <td>6.2</td><td>6.5</td><td>7.2</td><td>9.5</td><td>10.7</td><td>10.9</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">CPU %</td>
          <td>87</td><td>90</td><td>92</td><td>93</td><td>96</td><td>98</td>
        </tr>
        <tr>
          <td rowspan="3" style="border: 1px solid #ccc; padding: 8px 14px;">Spotify</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">Time (s)</td>
          <td>82.8</td><td>71.8</td><td>68.7</td><td>73.0</td><td>82.8</td><td>95.6</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">LLC Tp</td>
          <td>40.4</td><td>48.5</td><td>50.1</td><td>48.6</td><td>43.1</td><td>38.2</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">CPU %</td>
          <td>94</td><td>96</td><td>98</td><td>97</td><td>95</td><td>91</td>
        </tr>
        <tr>
          <td rowspan="3" style="border: 1px solid #ccc; padding: 8px 14px;">Graph500-28</td>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">Time (s)</td>
          <td>938.9</td><td>766.0</td><td>640.0</td><td>492.9</td><td>449.9</td><td>432.0</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">LLC Tp</td>
          <td>12.7</td><td>15.1</td><td>17.2</td><td>21.2</td><td>23.0</td><td>24.0</td>
        </tr>
        <tr>
          <td style="border: 1px solid #ccc; padding: 8px 14px;">CPU %</td>
          <td>70</td><td>80</td><td>87</td><td>92</td><td>95</td><td>96</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
<br>



## Prefetching at Lightspeed

// basic outline of the section: explain that prefetching is a common optimization technique to hide memory latency
// show for a variable length query returning paths, what perf record indicated the most CPU cycles were spent on 
// memory loads, meaning we are memory bound, and show what is the exact single variable that is causing CPU stalls
// then apply software prefetching on the variable with __builtin_prefetch and show the results and speedup




## So, what's the catch?


